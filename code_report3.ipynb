{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 : Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the digits dataset\n",
    "\n",
    "data_digits = np.load('digits.npz')\n",
    "\n",
    "x2_train = data_digits['xt'] / 255.0\n",
    "y2_train = data_digits['yt'].ravel()\n",
    "\n",
    "x2_test = data_digits['x'] / 255.0\n",
    "y2_test = data_digits['y'].ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pima dataset\n",
    "\n",
    "data_pima = np.load('pima.npz')\n",
    "\n",
    "matrix_names = data_pima.files\n",
    "for name in matrix_names:\n",
    "    print(name)\n",
    "\n",
    "x1 = data_pima['xall']\n",
    "y1 = data_pima['yall']\n",
    "varnames = data_pima['varnames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x1, columns=varnames)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y1 = pd.DataFrame(y1, columns=['RÃ©sultat'])\n",
    "df_y1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into 28x28 images\n",
    "x2_images = x2.reshape(-1, 28, 28)\n",
    "\n",
    "# Create a grid of image subplots\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x2_images[i], cmap='gray')\n",
    "    plt.title(f\"Class: {y2[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Predicting Diabetes on the Pima dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Know the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplots between pairs of variables with class coloring\n",
    "df['Class'] = y1\n",
    "sns.pairplot(df, hue='Class', palette='tab10')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the variables that seem to help predict the class? Do those variable make sense from a\n",
    "medical perspective ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the number of samples to keep for training/validation\n",
    "n_train_validation = 300\n",
    "\n",
    "# Split the data into training/validation and test sets\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "    df.drop('Class', axis=1), df['Class'], test_size=len(df) - n_train_validation, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Training/Validation Set (X):\", x_train_val.shape)\n",
    "print(\"Training/Validation Set (Y):\", y_train_val.shape)\n",
    "print(\"\\nTest Set (X):\", x_test.shape)\n",
    "print(\"Test Set (Y):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance of the original data\n",
    "mean_before_scaling = x_train_val.mean()\n",
    "variance_before_scaling = x_train_val.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistics before and after scaling\n",
    "print(\"Original Data - Mean:\")\n",
    "print(mean_before_scaling)\n",
    "print(\"Original Data - Variance:\")\n",
    "print(variance_before_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training/validation data\n",
    "x_train_val_scaled = scaler.fit_transform(x_train_val)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Check the variances of the scaled data\n",
    "print(\"Variances of scaled training/validation data:\")\n",
    "print(np.var(x_train_val_scaled, axis=0))\n",
    "\n",
    "print(\"Variances of scaled test data:\")\n",
    "print(np.var(x_test_scaled, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "num_features = x_train_val.shape[1]\n",
    "\n",
    "# Create a figure with subplots for all features\n",
    "fig, axes = plt.subplots(num_features, 2, figsize=(12, 2 * num_features))\n",
    "\n",
    "for feature_index in range(num_features):\n",
    "    feature_name = varnames[feature_index]  # Get the feature name\n",
    "    # Plot histograms of the feature before and after scaling\n",
    "    axes[feature_index, 0].hist(x_train_val.values[:, feature_index], bins=30, color='b', alpha=0.7, label='Original Data')\n",
    "    axes[feature_index, 0].set_title(f'Feature: {feature_name} - Before Scaling')\n",
    "    axes[feature_index, 0].legend()\n",
    "\n",
    "    axes[feature_index, 1].hist(x_train_val_scaled[:, feature_index], bins=30, color='r', alpha=0.7, label='Scaled Data')\n",
    "    axes[feature_index, 1].set_title(f'Feature: {feature_name} - After Scaling')\n",
    "    axes[feature_index, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bayesian decision and linear classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA classifier with default parameters\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the LDA classifier on the training data\n",
    "lda.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lda.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_default_lda = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) on the test data\n",
    "# First, get the probability estimates for class 1\n",
    "y_prob = lda.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_default_lda = roc_auc_score(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_default_lda:.2f}\")\n",
    "print(f\"AUC: {auc_default_lda:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the range of values for the 'solver' parameter\n",
    "param_grid = {'solver': ['svd', 'lsqr', 'eigen'],\n",
    "              'shrinkage': np.linspace(0,1,100)}  # Example: Different solver options\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(lda, param_grid, scoring=make_scorer(roc_auc_score), verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(x_train_val_scaled, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results of the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator and best parameter\n",
    "best_lda = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Evaluate the best estimator on the test data\n",
    "best_lda.fit(x_train_val_scaled, y_train_val)\n",
    "y_pred = best_lda.predict(x_test_scaled)\n",
    "y_prob_best_lda = best_lda.predict_proba(x_test_scaled)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_best_lda = accuracy_score(y_test, y_pred)\n",
    "auc_best_lda = roc_auc_score(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"accuracy_best_lda: {accuracy_best_lda:.2f}\")\n",
    "print(f\"auc_best_lda: {auc_best_lda:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QDA classifier with default parameters\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Fit the QDA classifier on the training data\n",
    "qda.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_qda = qda.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of QDA on the test data\n",
    "accuracy_default_qda = accuracy_score(y_test, y_pred_qda)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for QDA on the test data\n",
    "y_prob_qda = qda.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_qda = roc_auc_score(y_test, y_prob_qda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"QDA Accuracy: {accuracy_default_qda:.2f}\")\n",
    "print(f\"QDA AUC: {auc_default_qda:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QDA classifier\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Define the range of values for the 'reg_param' parameter\n",
    "param_grid = {'reg_param': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "# Create a scorer for ROC AUC\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(qda, param_grid, scoring=roc_auc_scorer, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best estimator and best parameter\n",
    "best_qda = grid_search.best_estimator_\n",
    "best_reg_param = grid_search.best_params_['reg_param']\n",
    "\n",
    "# Evaluate the best estimator on the test data\n",
    "best_qda.fit(x_train_val_scaled, y_train_val)\n",
    "y_prob_best_qda = best_qda.predict_proba(x_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_best_qda = accuracy_score(y_test, best_qda.predict(x_test_scaled))\n",
    "auc_best_qda = roc_auc_score(y_test, y_prob_best_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and the corresponding AUC\n",
    "print(f\"Best reg_param: {best_reg_param}\")\n",
    "print(f\"Best AUC: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare LDA and QDA results\n",
    "print(f\"accuracy_best_lda: {accuracy_best_lda:.2f}\")\n",
    "print(f\"auc_best_lda: {auc_best_lda:.2f}\")\n",
    "print(f\"accuracy_best_qda: {accuracy_best_qda:.2f}\")\n",
    "print(f\"auc_best_qda: {auc_best_qda:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian Naive Bayes (NB) classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the NB classifier on the training data\n",
    "gnb.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gnb = gnb.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of NB on the test data\n",
    "accuracy_default_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for NB on the test data\n",
    "y_prob_gnb = gnb.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_gnb = roc_auc_score(y_test, y_prob_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NB Accuracy: {accuracy_default_gnb:.2f}\")\n",
    "print(f\"NB AUC: {auc_default_gnb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the performance of QDA and LDA for comparison\n",
    "print(f\"QDA Accuracy: {accuracy_best_qda:.2f}\")\n",
    "print(f\"QDA AUC: {auc_best_qda:.2f}\")\n",
    "print(f\"LDA Accuracy: {accuracy_best_lda:.2f}\")\n",
    "print(f\"LDA AUC: {auc_best_lda:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the LR classifier on the training data\n",
    "lr.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_lr = lr.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of LR on the test data\n",
    "accuracy_default_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for LR on the test data\n",
    "y_prob_lr = lr.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_lr = roc_auc_score(y_test, y_prob_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LR Accuracy: {accuracy_default_lr:.2f}\")\n",
    "print(f\"LR AUC: {auc_default_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "# Define the range of values for the 'C' parameter\n",
    "param_grid = {'C': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# Create a scorer for ROC AUC\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(lr, param_grid, scoring=roc_auc_scorer, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best estimator and best parameter\n",
    "best_lr = grid_search.best_estimator_\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "# Evaluate the best estimator on the test data\n",
    "best_lr.fit(x_train_val_scaled, y_train_val)\n",
    "y_prob_best_lr = best_lr.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "accuracy_best_lr = accuracy_score(y_test, best_lr.predict(x_test_scaled))\n",
    "auc_best_lr = roc_auc_score(y_test, y_prob_best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and the corresponding AUC\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Best AUC: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the results\n",
    "print(f\"accuracy_best_lr: {accuracy_best_lr:.2f}\")\n",
    "print(f\"auc_best_lr: {auc_best_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the best decision method so far? Is the best model linear (LAD,LR) on quadratic (QDA,NB)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_linear = {\n",
    "\"LDA\" : [accuracy_default_lda, auc_default_lda, accuracy_best_lda, auc_best_lda],\n",
    "\"QDA\" : [accuracy_default_qda, auc_default_qda, accuracy_best_qda, auc_best_qda],\n",
    "\"GNB\" : [accuracy_default_gnb, auc_default_gnb, accuracy_default_gnb, auc_default_gnb],\n",
    "\"LR\" : [accuracy_default_lr, auc_default_lr, accuracy_best_lr, auc_best_lr]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_linear = pd.DataFrame(data_linear, index=[\"Accuracy Default\", \"AUC Default\", \"Accuracy Best\", \"AUC Best\"])\n",
    "\n",
    "# Applying a style to have a better result :)\n",
    "styled_df_linear = df_linear.style.set_properties(**{'text-align': 'center'})\n",
    "styled_df_linear.set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    "\n",
    "# Plot the dataframe\n",
    "styled_df_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each class in 1D\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# LDA\n",
    "plt.subplot(141)\n",
    "plt.hist(y_prob_best_lda[y_test == -1], color='blue', alpha=0.5, label='Class -1')\n",
    "plt.hist(y_prob_best_lda[y_test == 1], color='red', alpha=0.5, label='Class 1')\n",
    "plt.title('LDA Score Distribution')\n",
    "plt.xlabel('Scores')\n",
    "plt.legend()\n",
    "\n",
    "# QDA\n",
    "plt.subplot(142)\n",
    "plt.hist(y_prob_best_qda[y_test == -1], color='blue', alpha=0.5, label='Class -1')\n",
    "plt.hist(y_prob_best_qda[y_test == 1], color='red', alpha=0.5, label='Class 1')\n",
    "plt.title('QDA Score Distribution')\n",
    "plt.xlabel('Scores')\n",
    "plt.legend()\n",
    "\n",
    "# Gaussian Naive Bayes (GNB)\n",
    "plt.subplot(143)\n",
    "plt.hist(y_prob_gnb[y_test == -1], color='blue', alpha=0.5, label='Class -1')\n",
    "plt.hist(y_prob_gnb[y_test == 1], color='red', alpha=0.5, label='Class 1')\n",
    "plt.title('GNB Score Distribution')\n",
    "plt.xlabel('Scores')\n",
    "plt.legend()\n",
    "\n",
    "# Logistic Regression (LR)\n",
    "plt.subplot(144)\n",
    "plt.hist(y_prob_best_lr[y_test == -1], color='blue', alpha=0.5, label='Class -1')\n",
    "plt.hist(y_prob_best_lr[y_test == 1], color='red', alpha=0.5, label='Class 1')\n",
    "plt.title('LR Score Distribution')\n",
    "plt.xlabel('Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the weight for a good linear model. What is the effect of each variable on the risk of diabetes?\n",
    "Does it make medical sense?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Nonlinear methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the RF classifier on the training data\n",
    "rf.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of RF on the test data\n",
    "accuracy_default_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for RF on the test data\n",
    "y_prob_rf = rf.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_rf = roc_auc_score(y_test, y_prob_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RF default Accuracy: {accuracy_default_rf:.2f}\")\n",
    "print(f\"RF default AUC: {auc_default_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution to search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None] + list(randint(10, 30).rvs(5, random_state=42)),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 4),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                   n_iter=10, scoring=make_scorer(accuracy_score), cv=5, random_state=42)\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params = random_search.best_params_\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y_pred_rf = best_rf.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for RF on the test data\n",
    "y_prob_best_rf = best_rf.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_best_rf = roc_auc_score(y_test, y_prob_best_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"\\nBest RF Accuracy: {accuracy_best_rf:.2f}\")\n",
    "print(f\"Best RF AUC: {auc_best_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the SVC classifier on the training data\n",
    "svc.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svc = svc.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of SVC on the test data\n",
    "accuracy_default_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for SVC on the test data\n",
    "y_prob_svc = svc.decision_function(x_test_scaled)\n",
    "auc_default_svc = roc_auc_score(y_test, y_prob_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVC default Accuracy: {accuracy_default_svc:.2f}\")\n",
    "print(f\"SVC default AUC: {auc_default_svc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution to search\n",
    "param_dist = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel type\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(estimator=SVC(), param_distributions=param_dist,\n",
    "                                   n_iter=10, scoring='accuracy', cv=5, random_state=42)\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_svc = random_search.best_params_\n",
    "best_svc = random_search.best_estimator_\n",
    "\n",
    "# Train and evaluate the SVC with the best parameters\n",
    "y_pred_best_svc = best_svc.predict(x_test_scaled)\n",
    "accuracy_best_svc = accuracy_score(y_test, y_pred_best_svc)\n",
    "y_prob_best_svc = best_svc.decision_function(x_test_scaled)\n",
    "auc_best_svc = roc_auc_score(y_test, y_prob_best_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best SVC Parameters: {best_params_svc}\")\n",
    "print(f\"\\nBest SVC Accuracy: {accuracy_best_svc:.2f}\")\n",
    "print(f\"Best SVC AUC: {auc_best_svc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MLP classifier\n",
    "mlp = MLPClassifier(max_iter=10000)\n",
    "\n",
    "# Fit the MLP classifier on the training data\n",
    "mlp.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_mlp = mlp.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of MLP on the test data\n",
    "accuracy_default_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for MLP on the test data\n",
    "y_prob_mlp = mlp.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_mlp = roc_auc_score(y_test, y_prob_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MLP default Accuracy: {accuracy_default_mlp:.2f}\")\n",
    "print(f\"MLP default AUC: {auc_default_mlp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution to search\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 25)],\n",
    "    'activation': ['logistic', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'max_iter': [2000],\n",
    "}\n",
    "\n",
    "# Create an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=2000, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(estimator=mlp, param_distributions=param_dist,\n",
    "                                   n_iter=8, scoring='accuracy', cv=5, random_state=42)\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_mlp = random_search.best_params_\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y_pred_best_mlp = best_mlp.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_mlp = accuracy_score(y_test, y_pred_best_mlp)\n",
    "\n",
    "# Calculate AUC on the test data\n",
    "y_prob_best_mlp = best_mlp.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_best_mlp = roc_auc_score(y_test, y_prob_best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best MLP Parameters: {best_params_mlp}\")\n",
    "print(f\"\\nBest MLP Accuracy: {accuracy_best_mlp:.2f}\")\n",
    "print(f\"Best MLP AUC: {auc_best_mlp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradient Boosting classifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the GBC classifier on the training data\n",
    "gbc.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gbc = gbc.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy of GBC on the test data\n",
    "accuracy_default_gbc = accuracy_score(y_test, y_pred_gbc)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve) for GBC on the test data\n",
    "y_prob_gbc = gbc.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_default_gbc = roc_auc_score(y_test, y_prob_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GBC default Accuracy: {accuracy_default_gbc:.2f}\")\n",
    "print(f\"GBC default AUC: {auc_default_gbc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution to search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(estimator=gbc, param_distributions=param_dist,\n",
    "                                   n_iter=10, scoring='accuracy', cv=5, random_state=42)\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(x_train_val_scaled, y_train_val)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_gbc = random_search.best_params_\n",
    "best_gbc = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y_pred_best_gbc = best_gbc.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_gbc = accuracy_score(y_test, y_pred_best_gbc)\n",
    "\n",
    "# Calculate AUC on the test data\n",
    "y_prob_best_gbc = best_gbc.predict_proba(x_test_scaled)[:, 1]\n",
    "auc_best_gbc = roc_auc_score(y_test, y_prob_best_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best GBC Parameters: {best_params_gbc}\")\n",
    "print(f\"\\nBest GBC Accuracy: {accuracy_best_gbc:.2f}\")\n",
    "print(f\"Best GBC AUC: {auc_best_gbc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison and interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_linear = {\n",
    "\"RF\" : [accuracy_default_rf, auc_default_rf, accuracy_best_rf, auc_best_rf],\n",
    "\"SVC\" : [accuracy_default_svc, auc_default_svc, accuracy_best_svc, auc_best_svc],\n",
    "\"MLP\" : [accuracy_default_mlp, auc_default_mlp, accuracy_best_mlp, auc_best_mlp],\n",
    "\"GBC\" : [accuracy_default_gbc, auc_default_gbc, accuracy_best_gbc, auc_best_gbc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_non_linear = pd.DataFrame(data_non_linear, index=[\"Accuracy Default\", \"AUC Default\", \"Accuracy Best\", \"AUC Best\"])\n",
    "\n",
    "# Applying a style to have a better result :)\n",
    "styled_df_non_linear = df_non_linear.style.set_properties(**{'text-align': 'center'})\n",
    "styled_df_non_linear.set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    "\n",
    "df_methods = df_linear.join(df_non_linear, rsuffix='_non_linear')\n",
    "\n",
    "# Display the combined styled DataFrame\n",
    "df_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which model is best from a medical/practical perspective? Do we need non-linearity in this application?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fnr(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the False Negative Rate (FNR) from a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: numpy.ndarray\n",
    "        The confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - fnr: float\n",
    "        The False Negative Rate.\n",
    "    \"\"\"\n",
    "    TP = confusion_matrix[1, 1]  # True Positives\n",
    "    FN = confusion_matrix[1, 0]  # False Negatives\n",
    "\n",
    "    fnr = FN / (FN + TP)\n",
    "    return fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_model = best_lr.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_best_model)\n",
    "conf_matrix_plot = ConfusionMatrixDisplay(conf_matrix, display_labels=best_lr.classes_)\n",
    "\n",
    "FNR = calculate_fnr(conf_matrix)\n",
    "\n",
    "conf_matrix_plot.plot()\n",
    "print(f\"\\nFalse Negative Rate (FNR): {FNR:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since a false negative can have an important medical impact, propose a new threshold for the predicted\n",
    "score that leads to a FNR of less that 10% (this can be done by changing manually the value of the\n",
    "intercept_ in the trained classifier).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired FNR (e.g., 10% or 0.10)\n",
    "desired_fnr = 0.10\n",
    "\n",
    "# Calculate the new threshold based on the desired FNR\n",
    "threshold = -np.log(1/desired_fnr - 1)\n",
    "\n",
    "# Apply the threshold to the predicted probabilities\n",
    "y_pred_new_threshold = (best_lr.predict_proba(x_test_scaled)[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix with the new threshold\n",
    "conf_matrix_new_threshold = confusion_matrix(y_test, y_pred_new_threshold)\n",
    "\n",
    "# Calculate the new FNR with the modified threshold\n",
    "new_fnr = calculate_fnr(conf_matrix_new_threshold)\n",
    "print(f\"New False Negative Rate (FNR): {new_fnr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "print(\"Threshold : \", threshold)\n",
    "print(f\"\\nNew False Negative Rate (FNR): {new_fnr:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix with the new threshold\n",
    "conf_matrix_new_threshold_display = ConfusionMatrixDisplay(conf_matrix_new_threshold, display_labels=best_lr.classes_)\n",
    "conf_matrix_new_threshold_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Predicting Classes on the Digits dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X training shape : \",x2_train.shape)\n",
    "print(\"X test shape : \",x2_test.shape)\n",
    "print(\"\\nY training shape : \",y2_train.shape)\n",
    "print(\"Y test shape : \",y2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluate the different supervised methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the LDA model with default parameters\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x2_train, y2_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y2_pred = lda.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_default_lda_digits = accuracy_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LDA default Accuracy on Digits Dataset: {accuracy_default_lda_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid for LDA\n",
    "param_grid = {\n",
    "    'solver': ['lsqr', 'eigen'],\n",
    "}\n",
    "\n",
    "# Create an LDA model with 'lsqr' solver\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr')\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(x2_train, y2_train)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_lda_digits = grid_search.best_params_\n",
    "best_lda_digits = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y2_pred_best = best_lda_digits.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_lda_digits = accuracy_score(y2_test, y2_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best LDA Parameters: {best_params_lda_digits}\")\n",
    "print(f\"Best LDA Accuracy on Digits Dataset: {accuracy_best_lda_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the LR model with default parameters\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x2_train, y2_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y2_pred = lr.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_default_lr_digits = accuracy_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LR default Accuracy on Digits Dataset: {accuracy_default_lr_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reduced parameter grid for LR\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 1, 100],\n",
    "    'max_iter': [100, 1000],\n",
    "    'solver': ['liblinear'],\n",
    "}\n",
    "\n",
    "# Create an LR model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV instance with parallel processing\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(x2_train, y2_train)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_lr = grid_search.best_params_\n",
    "best_lr = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y2_pred_best = best_lr.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_lr_digits = accuracy_score(y2_test, y2_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best LR Parameters: {best_params_lr}\")\n",
    "print(f\"Best LR Accuracy on Digits Dataset: {accuracy_best_lr_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the SVC model with default parameters\n",
    "svc = SVC()\n",
    "svc.fit(x2_train, y2_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y2_pred = svc.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_default_svc_digits = accuracy_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVC default Accuracy on Digits Dataset: {accuracy_default_svc_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid for SVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "# Create an SVC model\n",
    "svc = SVC()\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(x2_train, y2_train)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_svc = grid_search.best_params_\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y2_pred_best_svc = best_svc.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_svc_digits = accuracy_score(y2_test, y2_pred_best_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best SVC Parameters: {best_params_svc}\")\n",
    "print(f\"Best SVC Accuracy on Digits Dataset: {accuracy_best_svc_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the MLP model with default parameters\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(x2_train, y2_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y2_pred = mlp.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_default_mlp_digits = accuracy_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MLP default Accuracy on Digits Dataset: {accuracy_default_mlp_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution to search\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 25)],\n",
    "    'activation': ['logistic', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'max_iter': [2000],\n",
    "}\n",
    "\n",
    "# Create an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=2000, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(estimator=mlp, param_distributions=param_dist,\n",
    "                                   n_iter=8, scoring='accuracy', cv=5, random_state=42)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(x2_train, y2_train)\n",
    "\n",
    "# Get the best parameters and the corresponding model\n",
    "best_params_mlp = grid_search.best_params_\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data with the best model\n",
    "y2_pred_best_mlp = best_mlp.predict(x2_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy_best_mlp_digits = accuracy_score(y2_test, y2_pred_best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best MLP Parameters: {best_params_mlp}\")\n",
    "print(f\"Best MLP Accuracy on Digits Dataset: {accuracy_best_mlp_digits:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Interpreting the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Convolutional Neural network (CNN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
